# UWSS â€“ Universal Web Scraping System

A universal, config-driven academic data harvesting and web scraping system designed to work with any academic database, research repository, or web source. The system uses a plugin-based adapter architecture that allows switching between sources or topics by changing configuration only, without rewriting the pipeline.

## Overview

UWSS is built to be **truly universal**:
- **Multi-source support**: Works with structured databases (arXiv, TRB/TRID, Crossref, OpenAlex) via official APIs/protocols, and unstructured web content (research group websites, personal pages, scattered PDFs) via intelligent crawling
- **Universal pipeline**: The same pipeline (discover â†’ score â†’ export â†’ fetch â†’ extract) works for any source; only the discovery adapter differs
- **Config-driven**: Switch topics or sources by editing `config/config.yaml`; no code changes required
- **Policy-compliant**: Respects `robots.txt`, Terms of Service, and uses official channels (OAI-PMH, REST APIs) when available
- **Agent-ready architecture**: Designed to support AI/agent-based autonomous discovery and refinement in future phases

## Key Features

- **Database-first architecture**: Postgres (production) or SQLite (local) as single source of truth
- **Idempotent operations**: Safe to rerun; checkpoints and upserts prevent duplicate work
- **Comprehensive metadata extraction**: Title, abstract, authors, affiliations, keywords, DOI, access flags (open access, paywall, abstract-only)
- **Quality assurance**: Keyword-based relevance scoring with negative keywords, sampling tools for manual review
- **Reproducible**: Version-pinned URLs, SHA256 checksums, sidecar metadata files
- **Observable**: Structured JSON logs, metrics, validation tools

## Architecture

### Universal Pipeline

```
DISCOVER â†’ SCORE â†’ EXPORT â†’ FETCH â†’ EXTRACT
   â†‘
   â””â”€ Source-specific adapters (OAI-PMH, REST API, sitemap crawler, web spider)
```

### Adapter Pattern

Each source has its own discovery adapter, but all sources share the same pipeline:

- **arXiv**: OAI-PMH adapter â†’ official metadata harvesting
- **TRB/TRID**: Sitemap crawler â†’ parse sitemap.xml, crawl HTML pages
- **Web of Science/Scopus**: REST API adapter (requires institutional subscription + API keys)
- **Web crawling**: Scrapy-based spider for research groups, personal pages, scattered PDFs

After discovery, all documents flow through the same pipeline:
- **Score**: Keyword-based relevance scoring (configurable positive/negative keywords)
- **Export**: Filter and export to JSONL/CSV with various criteria
- **Fetch**: Download PDFs with retry/backoff, rate limiting, integrity checks
- **Extract**: Full-text extraction (GROBID, local PDF parsing)

### Database Schema

All sources map to the same universal `Document` model:

## Project Structure

```
uwss/
â”œâ”€â”€ config/                 # Configuration files
â”‚   â”œâ”€â”€ config.yaml        # Main configuration
â”‚   â””â”€â”€ keywords*.txt      # Keyword files
â”œâ”€â”€ data/                  # Production data and PDFs
â”‚   â”œâ”€â”€ paperscraper_export.jsonl
â”‚   â”œâ”€â”€ new_sources_final.jsonl
â”‚   â””â”€â”€ paperscraper_pdfs/
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ development/       # Development docs
â”‚   â”œâ”€â”€ integration/       # Integration guides
â”‚   â”œâ”€â”€ project/          # Project docs
â”‚   â””â”€â”€ reports/          # Analysis reports
â”œâ”€â”€ scripts/               # Utility scripts
â”‚   â”œâ”€â”€ analysis/         # Data analysis scripts
â”‚   â”œâ”€â”€ testing/          # Testing scripts
â”‚   â””â”€â”€ utilities/        # Maintenance scripts
â”œâ”€â”€ src/uwss/             # Main source code
â”‚   â”œâ”€â”€ cli/              # Command-line interface
â”‚   â”œâ”€â”€ sources/          # Source adapters
â”‚   â””â”€â”€ [modules]/        # Core modules
â”œâ”€â”€ test/                 # Test artifacts and data
â”‚   â”œâ”€â”€ outputs/          # Test results
â”‚   â”œâ”€â”€ metrics/          # Performance metrics
â”‚   â”œâ”€â”€ databases/        # Test databases
â”‚   â””â”€â”€ reports/          # Test reports
â””â”€â”€ tests/                # Unit/integration tests
```
- Identification: `title`, `abstract`, `authors`, `affiliations`, `keywords`, `doi`, `year`
- Source tracking: `source`, `source_url`, `landing_url`
- Access flags: `oa_status`, `pdf_url`, `pdf_status`
- Scoring: `relevance_score`, `keywords_found`
- Files: `local_path`, `content_path`, `checksum_sha256`

## Quick Start

### Prerequisites

- Python 3.8+
- (Optional) Postgres for production use
- (Optional) GROBID service for advanced PDF parsing

### Installation

```bash
# Clone repository
git clone <repository-url>
cd uwss

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Basic Workflow (arXiv Example)

1. **Validate configuration**
```bash
python -m src.uwss.cli config-validate --config config/config.yaml
```

2. **Harvest metadata** (arXiv via OAI-PMH)
```bash
python -m src.uwss.cli arxiv-harvest-oai \
  --from 2024-10-01 --max 100 --resume \
  --metrics-out data/runs/arxiv_h.json
```

3. **Score by relevance**
```bash
python -m src.uwss.cli score-keywords \
  --config config/config.yaml \
  --db data/uwss.sqlite
```

4. **Export filtered results**
```bash
python -m src.uwss.cli export \
  --db data/uwss.sqlite \
  --out data/export/filtered.jsonl \
  --require-match --year-min 1995 \
  --ids-out data/export/filtered_ids.txt
```

5. **Fetch PDFs** (only for exported IDs)
```bash
python -m src.uwss.cli arxiv-fetch-pdf \
  --ids-file data/export/filtered_ids.txt \
  --limit 200 \
  --metrics-out data/runs/arxiv_p.json
```

6. **Extract full text**
```bash
python -m src.uwss.cli extract-full-text \
  --db data/uwss.sqlite \
  --content-dir data/content \
  --limit 200
```

### Switching Sources

To use a different source, simply change the discovery command:

```bash
# TRB/TRID (sitemap crawling - coming soon)
python -m src.uwss.cli trid-discover-sitemap --max 100

# After discovery, same pipeline:
python -m src.uwss.cli score-keywords --config config/config.yaml
python -m src.uwss.cli export --require-match
python -m src.uwss.cli fetch-pdfs --ids-file ids.txt
```

## Configuration

Edit `config/config.yaml` to customize:

- **Domain keywords**: Positive keywords for relevance scoring
- **Negative keywords**: Terms to penalize/exclude (e.g., physics terms for civil engineering focus)
- **Rate limits**: Throttling and jitter for polite crawling
- **Contact info**: Email and User-Agent for compliance

Example:
```yaml
domain_keywords:
  - "concrete deterioration"
  - "chloride diffusion"
  - "corrosion"

negative_keywords:
  - "quantum"
  - "neural network"
  - "machine learning"

rate_limits:
  throttle_sec: 1.0
  jitter_sec: 0.5
```

## Data Storage

- **Database**: `data/uwss.sqlite` (or Postgres via `--db-url`)
  - Tables: `documents`, `visited_urls`, `ingestion_state`
- **PDFs**: `data/files/{source}_{id}.pdf` with sidecar `{source}_{id}.meta.json`
- **Content**: `data/content/doc_{id}.txt` (extracted full text)
- **Metrics**: `data/runs/*.json` (harvest/fetch/extract metrics)
- **Policy snapshots**: `docs/policies/{source}/` (compliance artifacts)

## Monitoring & Quality Assurance

### Recent Downloads
```bash
python -m src.uwss.cli recent-downloads \
  --hours 1 --limit 10 \
  --source arxiv \
  --json-out data/runs/recent.json
```

### Sampling for Manual Review
```bash
python -m src.uwss.cli sample-records \
  --db data/uwss.sqlite \
  --out data/samples/manual_review.jsonl \
  --n 50 --pdf-only --require-match
```

### Validation & Statistics
```bash
python -m src.uwss.cli validate \
  --db data/uwss.sqlite \
  --json-out data/export/validation.json

python -m src.uwss.cli stats \
  --db data/uwss.sqlite \
  --json-out data/export/stats.json
```

## Scripts & Utilities

The project includes comprehensive scripts for analysis, testing, and utilities:

### Analysis Scripts (`scripts/analysis/`)
```bash
# Data quality checking and visualization
python scripts/analysis/check_paperscraper_data.py    # Validate Paperscraper data
python scripts/analysis/show_source_summary.py        # Show source statistics
python scripts/analysis/view_scale_test_results.py    # View test results

# Data exploration and sampling
python scripts/analysis/show_detailed_metadata.py     # Detailed metadata view
python scripts/analysis/quick_analysis.py             # Quick data analysis
```

### Testing Scripts (`scripts/testing/`)
```bash
# Component testing
python scripts/testing/test_full_pipeline.py          # Full pipeline test
python scripts/testing/test_paperscraper_discovery.py # Paperscraper discovery test
python scripts/testing/run_scale_test.py              # Large-scale testing

# API and integration testing
python scripts/testing/test_pyalex_direct.py          # OpenAlex API test
python scripts/testing/test_semanticscholar_direct.py # Semantic Scholar API test
```

### Utility Scripts (`scripts/utilities/`)
```bash
# Data maintenance and fixes
python scripts/utilities/fix_year_in_database.py      # Fix year data issues
python scripts/utilities/create_viewer_files.py       # Generate viewer files
```

## Current Status

### âœ… **FULLY IMPLEMENTED & OPERATIONAL**

#### **Multi-Source Academic Database Integration**
- **âœ… Paperscraper (arXiv + PubMed)**: Complete integration with 269 harvested documents, 40+ PDFs downloaded
- **âœ… Crossref**: Full API integration with habanero library, HTML cleaning, 268 documents harvested
- **âœ… Semantic Scholar**: Complete API integration with semantic_scholar library, 283 documents harvested
- **âœ… OpenAlex**: Technical integration complete (pyalex library), database coverage analysis performed
- **âœ… DOAJ**: Directory of Open Access Journals integration ready

#### **Universal Pipeline - COMPLETE**
- **âœ… DISCOVER**: All sources working with modular adapters
- **âœ… SCORE**: Keyword-based relevance scoring with positive/negative keywords
- **âœ… EXPORT**: JSONL/CSV export with comprehensive metadata
- **âœ… FETCH**: PDF downloading with retry logic, 40+ PDFs successfully downloaded
- **âœ… EXTRACT**: Full-text extraction from PDFs (framework ready)

#### **Database & Storage**
- **âœ… SQLite/PostgreSQL**: Dual database support implemented and tested
- **âœ… Deduplication**: DOI/URL/title-based duplicate prevention
- **âœ… Data Quality**: HTML cleaning, normalization, validation
- **âœ… File Management**: Organized storage with checksums

#### **Quality Assurance & Monitoring**
- **âœ… Comprehensive Testing**: 30+ test scripts across analysis, testing, utilities
- **âœ… Metrics Collection**: Performance tracking and analysis tools
- **âœ… Data Validation**: Quality checks and sampling tools
- **âœ… Error Handling**: Robust retry logic and exception management

#### **Professional Organization**
- **âœ… Modular Architecture**: Clean separation of concerns
- **âœ… Script Organization**: 30 scripts categorized in `scripts/` directory
- **âœ… Documentation**: Comprehensive docs in `docs/` with multiple categories
- **âœ… Test Management**: Organized test artifacts in `test/` directory
- **âœ… GitHub Integration**: Complete repository with CI/CD ready

### ğŸš§ **IN PROGRESS**

- **TRB/TRID integration**: Sitemap crawling adapter (identified correct approach)
- **Web crawling expansion**: Research groups, personal pages, scattered PDFs
- **Researcher finder**: Extract researcher info, ORCID integration

### ğŸ“‹ **READY FOR EXTENSION**

- **Subscription databases**: Web of Science, Scopus, ScienceDirect (requires institutional access + API keys)
- **Agent framework**: LLM-based autonomous discovery and refinement
- **Cloud deployment**: AWS-hosted platform with usage instructions

## Compliance & Policy

UWSS is designed to be policy-compliant:

- **Official channels only**: Uses OAI-PMH, REST APIs, sitemap crawling (not unauthorized scraping)
- **Respects robots.txt**: Checks and honors robots.txt before crawling
- **Rate limiting**: Configurable throttling and jitter to avoid overwhelming servers
- **Policy snapshots**: Stores compliance artifacts (Identify responses, robots.txt, links)
- **Terms of Service**: Subscription databases only integrated with proper institutional access and API credentials

## Roadmap

### Phase 1: Core Infrastructure âœ… **COMPLETED**
- âœ… Universal pipeline architecture (DISCOVER â†’ SCORE â†’ EXPORT â†’ FETCH â†’ EXTRACT)
- âœ… arXiv integration (OAI-PMH) with batch processing and retry logic
- âœ… Generic adapters (OAI-PMH, RSS/Atom parsers)
- âœ… Database-first architecture (SQLite/PostgreSQL)
- âœ… Modular CLI system with comprehensive commands

### Phase 2: Multi-Source Support âœ… **COMPLETED**
- âœ… **Paperscraper**: arXiv + PubMed integration (269 documents, 40+ PDFs)
- âœ… **Crossref**: Full API integration with habanero (268 documents)
- âœ… **Semantic Scholar**: Complete API integration (283 documents)
- âœ… **OpenAlex**: Technical integration complete with coverage analysis
- âœ… **DOAJ**: Directory integration framework ready
- ğŸš§ TRB/TRID (sitemap crawling - identified approach)

### Phase 3: Web Crawling Expansion ğŸš§ **IN PROGRESS**
- âœ… Research paper PDF discovery and downloading (40+ PDFs collected)
- ğŸš§ Research group website discovery (framework ready)
- ğŸš§ Personal faculty page crawling (extractors implemented)
- ğŸš§ Scattered PDF discovery (seed finder implemented)
- âœ… Search API integration (multiple sources integrated)

### Phase 4: Researcher & Group Finder ğŸ“‹ **READY**
- âœ… Extract researcher information (extractors implemented)
- ğŸš§ ORCID integration (framework ready)
- ğŸš§ Contact info extraction (extractors implemented)
- ğŸš§ Institution tracking (framework ready)

### Phase 5: Agent Framework ğŸ“‹ **PLANNED**
- ğŸš§ LLM orchestration (architecture designed)
- ğŸš§ Autonomous discovery (framework ready)
- ğŸš§ Iterative refinement (pipeline supports)
- ğŸš§ Tool integration (CLI extensible)

### Phase 6: Production & Scale ğŸš§ **IN PROGRESS**
- âœ… Professional project organization (scripts/, docs/, test/ structure)
- âœ… GitHub integration with CI/CD ready
- ğŸš§ Cloud deployment (Docker, AWS ready)
- ğŸš§ Performance optimization (indexes, caching)
- ğŸš§ Monitoring dashboard (metrics collection ready)

## Contributing

This project follows a modular, adapter-based architecture that has been successfully implemented for 5+ sources. To add a new source:

### âœ… **Proven Implementation Pattern**

1. **Create discovery adapter** in `src/uwss/sources/{source_name}/`
   - Implement `discover_{source_name}()` function
   - Use existing libraries (habanero, pyalex, semanticscholar, paperscraper) when available
   - Follow the `Document` schema mapping

2. **Add CLI command** in `src/uwss/cli/commands/`
   - Register command in `cli.py`
   - Follow existing command patterns
   - Add proper argument validation

3. **Create mapper** if needed
   - Map source-specific fields to universal `Document` schema
   - Handle data normalization (HTML cleaning, empty strings to None)

4. **Test integration**
   - Use scripts in `scripts/testing/` for validation
   - Run analysis scripts in `scripts/analysis/` to verify data quality
   - Add comprehensive tests in `tests/integration/`

5. **Document compliance**
   - Add policy compliance info in `docs/policies/`
   - Document API requirements and limitations

### ğŸ“š **Available Examples**
- **Paperscraper**: `src/uwss/sources/paperscraper/`
- **Crossref**: `src/uwss/sources/crossref_lib/`
- **Semantic Scholar**: `src/uwss/sources/semantic_scholar_lib/`
- **OpenAlex**: `src/uwss/sources/openalex_lib/`

### ğŸ› ï¸ **Development Scripts**
Use organized scripts for development:
- `scripts/testing/` - Test new integrations
- `scripts/analysis/` - Validate data quality
- `scripts/utilities/` - Maintenance and fixes

## License

[Specify license]

## Contact

[Specify contact information]
