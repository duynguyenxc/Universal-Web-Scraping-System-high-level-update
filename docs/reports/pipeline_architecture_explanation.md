# Pipeline Architecture: arXiv vs TRB

## ğŸ¯ CÃ¢u tráº£ lá»i ngáº¯n gá»n

**Pipeline VáºªN GIá»NG NHAU** - chá»‰ khÃ¡c á»Ÿ bÆ°á»›c **DISCOVER** (adapter).

- **arXiv**: OAI-PMH adapter â†’ gá»i API, parse XML
- **TRB**: Sitemap crawler/spider â†’ parse sitemap, crawl HTML pages
- **Sau discover**: Táº¥t cáº£ Ä‘á»u Ä‘i qua cÃ¹ng pipeline (score â†’ export â†’ fetch â†’ extract)

## ğŸ“Š SÆ¡ Ä‘á»“ Pipeline Universal

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UNIVERSAL PIPELINE                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DISCOVER   â”‚  â† CHá»ˆ BÆ¯á»šC NÃ€Y KHÃC NHAU
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â”œâ”€ arXiv: OAI-PMH adapter
      â”‚   â””â”€> Gá»i https://export.arxiv.org/oai2?verb=ListRecords
      â”‚   â””â”€> Parse XML metadata
      â”‚   â””â”€> Insert Document vÃ o DB
      â”‚
      â”œâ”€ TRB: Sitemap crawler
      â”‚   â””â”€> Parse sitemap.xml
      â”‚   â””â”€> Crawl HTML pages (Scrapy spider)
      â”‚   â””â”€> Extract metadata tá»« HTML
      â”‚   â””â”€> Insert Document vÃ o DB
      â”‚
      â””â”€ Web of Science: API adapter (náº¿u cÃ³ subscription)
          â””â”€> Gá»i REST API vá»›i API key
          â””â”€> Parse JSON response
          â””â”€> Insert Document vÃ o DB

      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    SCORE     â”‚  â† GIá»NG NHAU (keyword scoring)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    EXPORT    â”‚  â† GIá»NG NHAU (filter, export JSONL)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    FETCH     â”‚  â† GIá»NG NHAU (download PDFs)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXTRACT    â”‚  â† GIá»NG NHAU (GROBID, full-text)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” Chi tiáº¿t tá»«ng bÆ°á»›c

### 1. DISCOVER (Adapter Pattern)

**arXiv - OAI-PMH Adapter:**
```python
# src/uwss/arxiv/harvest_oai.py
def harvest_arxiv_oai(...):
    # Gá»i OAI-PMH API
    url = "https://export.arxiv.org/oai2?verb=ListRecords&metadataPrefix=oai_dc"
    response = requests.get(url)
    xml = parse_xml(response.text)
    
    # Parse metadata
    for record in xml.find_all('record'):
        doc = Document(
            title=record.find('title').text,
            abstract=record.find('description').text,
            # ...
        )
        # Insert vÃ o DB
        session.merge(doc)
```

**TRB - Sitemap Crawler:**
```python
# src/uwss/discovery/sitemap.py (hoáº·c Scrapy spider)
def discover_trb_sitemap(...):
    # Parse sitemap.xml
    sitemap_url = "https://trid.trb.org/sitemap.xml"
    sitemap = parse_sitemap(sitemap_url)
    
    # Crawl tá»«ng URL
    for url in sitemap.urls:
        html = requests.get(url).text
        doc = extract_metadata_from_html(html)  # Parse HTML
        # Insert vÃ o DB
        session.merge(doc)
```

**Káº¿t quáº£:** Cáº£ hai Ä‘á»u insert `Document` objects vÃ o **cÃ¹ng má»™t DB schema**.

### 2. SCORE (Giá»‘ng nhau)

```python
# src/uwss/score/__init__.py
def score_documents(session, config):
    # Query Táº¤T Cáº¢ documents (khÃ´ng phÃ¢n biá»‡t source)
    docs = session.query(Document).all()
    
    for doc in docs:
        # Score báº±ng keywords (giá»‘ng nhau cho má»i source)
        score = calculate_relevance_score(
            doc.title, 
            doc.abstract, 
            positive_keywords,
            negative_keywords
        )
        doc.relevance_score = score
```

### 3. EXPORT (Giá»‘ng nhau)

```python
# src/uwss/cli.py - export command
def cmd_export(args):
    # Query Táº¤T Cáº¢ documents (khÃ´ng phÃ¢n biá»‡t source)
    query = session.query(Document)
    if args.require_match:
        query = query.filter(Document.relevance_score > 0)
    
    # Export JSONL (giá»‘ng nhau cho má»i source)
    for doc in query:
        write_jsonl(doc)
```

### 4. FETCH (Giá»‘ng nhau)

```python
# src/uwss/fetch/arxiv_pdf.py (hoáº·c generic fetcher)
def fetch_pdfs(session, ids):
    for doc_id in ids:
        doc = session.query(Document).get(doc_id)
        
        # Download PDF (logic giá»‘ng nhau)
        pdf_url = doc.pdf_url or doc.landing_url
        download_pdf(pdf_url, output_path)
        
        # Update DB (giá»‘ng nhau)
        doc.local_path = output_path
        doc.pdf_status = 'downloaded'
```

## ğŸ—ï¸ Kiáº¿n trÃºc Universal

### Adapter Pattern

```python
# src/uwss/sources/
â”œâ”€â”€ arxiv/
â”‚   â”œâ”€â”€ harvest_oai.py      # OAI-PMH adapter
â”‚   â””â”€â”€ fetch_pdf.py         # arXiv-specific PDF fetcher
â”‚
â”œâ”€â”€ trb/
â”‚   â”œâ”€â”€ discover_sitemap.py  # Sitemap crawler adapter
â”‚   â””â”€â”€ parse_html.py        # HTML parser
â”‚
â””â”€â”€ generic/
    â”œâ”€â”€ oai_adapter.py       # Generic OAI-PMH (cho báº¥t ká»³ source nÃ o)
    â”œâ”€â”€ rss_adapter.py       # Generic RSS/Atom
    â””â”€â”€ sitemap_adapter.py   # Generic sitemap crawler
```

### Database Schema (Chung cho táº¥t cáº£)

```python
# src/uwss/store/models.py
class Document(Base):
    # Identification
    id = Column(Integer, primary_key=True)
    title = Column(String(500))
    abstract = Column(Text)
    authors = Column(Text)
    doi = Column(String(255))
    
    # Source tracking
    source = Column(String(50))  # 'arxiv', 'trb', 'wos', ...
    source_url = Column(String(1000))
    
    # Scoring
    relevance_score = Column(Float)
    
    # Files
    pdf_url = Column(String(1000))
    local_path = Column(String(1000))
    pdf_status = Column(String(50))
    
    # ... (táº¥t cáº£ sources dÃ¹ng cÃ¹ng schema)
```

## âœ… Káº¿t luáº­n

1. **Pipeline VáºªN GIá»NG NHAU**: discover â†’ score â†’ export â†’ fetch â†’ extract
2. **Chá»‰ khÃ¡c á»Ÿ DISCOVER**: má»—i source cÃ³ adapter riÃªng
   - arXiv: OAI-PMH adapter
   - TRB: Sitemap crawler/spider
   - Web of Science: REST API adapter (náº¿u cÃ³ subscription)
3. **Sau discover**: Táº¥t cáº£ documents vÃ o cÃ¹ng DB schema â†’ cÃ¹ng pipeline
4. **Kiáº¿n trÃºc universal**: ThÃªm source má»›i = thÃªm adapter má»›i, khÃ´ng cáº§n sá»­a pipeline

## ğŸ¯ VÃ­ dá»¥ thá»±c táº¿

**arXiv:**
```bash
# Discover (OAI-PMH)
python -m src.uwss.cli arxiv-harvest-oai --max 100

# Score (chung)
python -m src.uwss.cli score-keywords --config config.yaml

# Export (chung)
python -m src.uwss.cli export --require-match

# Fetch (chung)
python -m src.uwss.cli arxiv-fetch-pdf --ids-file ids.txt
```

**TRB:**
```bash
# Discover (Sitemap crawler)
python -m src.uwss.cli trb-discover-sitemap --max 100

# Score (chung) - GIá»NG NHAU
python -m src.uwss.cli score-keywords --config config.yaml

# Export (chung) - GIá»NG NHAU
python -m src.uwss.cli export --require-match

# Fetch (chung) - GIá»NG NHAU
python -m src.uwss.cli fetch-pdfs --ids-file ids.txt
```

**â†’ Sau bÆ°á»›c discover, pipeline hoÃ n toÃ n giá»‘ng nhau!**

